{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Predicting the survival of passengers of the titanic disaster with machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise aims to predict the outcome of the titanic disaster using machine learning. The given data is split into training and test data. The train.csv will be used to fit the machine learning algorithms which are then applied on the test.csv to predict who will surivive. The datasets consist of different characteristics that are listed below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Variable | Definition | Key |\n",
    "| :- | -: | :-: |\n",
    "| PClass | Passenger Class | 1=1st, 2=2nd, 3=3rd |\n",
    "| Sex | Gender | |\n",
    "| Age | Age in years |\n",
    "| SibSp | # of siblings / spouses aboard the titanic  |\n",
    "| Parch | # of parents / children aboard the titanic |\n",
    "| Ticket | Ticket number |\n",
    "| Fare | Passerger fare |\n",
    "| Cabin | Cabin number | \n",
    "| Embarked | Port of boarding the ship | C = Cherbourg, Q = Queenstown, S = Southampton |\n",
    "| Survived | Survival | 0=No, 1=Yes |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable Notes\n",
    "\n",
    "pclass:\n",
    "1st = Upper,\n",
    "2nd = Middle,\n",
    "3rd = Lower\n",
    "\n",
    "age: The age is fractional if less than 1. If the age is estimated, it will have the form xx.5\n",
    "\n",
    "sibsp: <br> The dataset defines family relations in this way: <br>\n",
    "Sibling = brother, sister, stepbrother, stepsister <br>\n",
    "Spouse = husband, wife (mistresses and fianc√©s were ignored)\n",
    "\n",
    "parch: <br> The dataset defines family relations in this way: <br>\n",
    "Parent = mother, father <br> Child = daughter, son, stepdaughter, stepson <br> Some children travelled only with a nanny, therefore parch=0 for them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise is split into three parts with data processing being the longest and most important part.\n",
    "1. Data vizualization\n",
    "2. Data processing\n",
    "3. Machine learning algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to import all libraries that we will need at a later point of the exercise. Seaborn and matplotlib are used for data vizulization. Pandas and numpy will help with data processing and linear algebra. At last the databases for the different machine learing algorithms are imported from sklearn. Optionally an image database is added for the purpose of addind pictures into the Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import necessary libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data vizualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part data vizualization is there to get an understanding of the data set. Only by knowing what we are working with, we can determine what data we need or don't. <br>\n",
    "With all libraries imported we can start by cearting new variables \"test_df\" which will contain our test set and trand_df which includes the training set. To do this we use the pandas function \"read_csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data sets one for testing one for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Look at the data structure (Colums and data types)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .info() function in Python shows us information regarding the data set. It displays all columns, the count of entires inside the columns and what type they have. In this case we have two float64, five int64 and five objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Have a look to the statistics of each column of the training dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Have a closer look to the first 10 rows of the training dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By taking a quick look at the training set, some issues are noticable. There are enties __missing__ (NaN = not a number) which have to be dealt with. Furthermore a lot of entries __are not numeric__, which means they have to converted __into numeric values__ for the machine learning algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a closer look at what's actually missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the NaN or empty entries in training dataset, calculate the total number, the relative number of missing data and display the first 5 rows\n",
    "\n",
    "#total = train_df.isnull().sum().sort_values(ascending=False)\n",
    "#percent_1 = train_df.isnull().sum()/train_df.isnull().count()*100\n",
    "#percent_2 = (round(percent_1, 1)).sort_values(ascending=False)\n",
    "#missing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\n",
    "#missing_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With only two missing values the Embarked feature can easily be filled. The Age feature is not that simple to fill and the Cabin feature with 687 missing values will probably be dropped alltogether. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use print() to have a look to the values of the colums again and answer the question in your group ! \n",
    "## Fill the gaps in the following Python cells and the pre coded parts(_____)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: <br>\n",
    "Looking again at all the features provided, which do you think won't have an affect on the prediction models? <br>\n",
    "\n",
    "Answer: <br>\n",
    "\n",
    "???\n",
    "\n",
    "Let's beginn by looking at the features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age & Sex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the graphical display of the Age and Sex regarding the survival rate, we will be using a histgram. A histogram shows the distributio of a specific feature. In this case it is the distribution of female and male passengers regarding their age and how many survived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#survived = 'survived'\n",
    "#not_survived = 'not survived'\n",
    "#fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\n",
    "\n",
    "#_____ Here you need the data from your training data set which are 'male'\n",
    "#_____ Here you need the data from your training data set which are 'female' then the following code works\n",
    "\n",
    "#ax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False)\n",
    "#ax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)\n",
    "#ax.legend()\n",
    "#ax.set_title('_____')\n",
    "#ax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\n",
    "#ax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False)\n",
    "#ax.legend()\n",
    "#_ = ax.set_title('_____')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By creating the histogram, it is clearly visibly that men at the age of 20 - 40 had a higher chance of survival. For women the highest chance of survival is between 14 and 40. It is also noticable that infants have a higher survivability rate. \n",
    "\n",
    "Question: <br>\n",
    "What can be done with the age to simplify it even more?\n",
    "\n",
    "Answer: <br>\n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embarked, Pclass and Sex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the feature Embarked we will be using FacetGrid. It helps in displaying distributions or relationships between multiple variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualise the ratio of survived male and female persons according to the PClass for each embarking \"S\", \"C\", \"Q\"- use a pointplot\n",
    "\n",
    "#FacetGrid = _______ (train_df, row='Embarked', height=4.5, aspect=1.6)\n",
    "#FacetGrid.map( ______ , 'Pclass', 'Survived', 'Sex', palette=None,  order=None, hue_order=None )\n",
    "#FacetGrid.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Embarked feature also shows to have a big impact on the survival chance of the passengers. Men that embarked on port C have a higher chance of survival than port S or Q. Women on the other hand have a low survival rate on port C and high chances at port S and Q. <br>\n",
    "The class also seems to have an affect on the survival rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualise the ratio of survived persons according to the PClass they are in - use a barplot\n",
    "\n",
    "#_____ (x='____', y='____', data=_____)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is immediately evident that class 1 has the most survivors and class 3 the least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize a histogram map of the ages for each class and survival feature (=1 or =0)\n",
    "\n",
    "#grid = _______ (train_df, col='_____', row='_____', height=2.2, aspect=1.6)\n",
    "#grid.map(_______, '_____', alpha=.5, bins=20)\n",
    "#grid.add_legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows the influence of Pclass and highlights the high mortality rate of passengers in class 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SibSp and Parch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SibSp and Parch are features that both show passengers and how many relatives they have on board. Consequently this feature should be combined into a single one. The followong code creates a new feature \"not_alone\" that shows if a passengers is not alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = [train_df, test_df]\n",
    "#for dataset in data:\n",
    "#    dataset['relatives'] = dataset['SibSp'] + dataset['Parch']\n",
    "#    dataset.loc[dataset['relatives'] > 0, 'not_alone'] = 0\n",
    "#    dataset.loc[dataset['relatives'] == 0, 'not_alone'] = 1\n",
    "#    dataset['not_alone'] = dataset['not_alone'].astype(int)\n",
    "\n",
    "##Display the number of passengers, who are not alone and those who are alome from the training dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the ratio of survived passengers with respect to the number of relatives - use a pointplot\n",
    "\n",
    "#axes = _______ ('______','______', ______ , aspect = 2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows the survival rate in regards of the relatives a person has. \n",
    "\n",
    "Question: \n",
    "\n",
    "Passengers with how many relatives have higher chances of survival ?\n",
    "\n",
    "Answer: \n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Concluding the data vizulization we:__\n",
    "- Looked for empty or missing values (NaN). Have to be dealt with later.\n",
    "    - Cabin 687 \n",
    "    - Age 177\n",
    "    - Embarked 2\n",
    "- Searched for values that have an influence on the survival chance\n",
    "    - Age and Sex both have impact on the survial\n",
    "    - Embarked and Pclass also show effect on the survival \n",
    "- SibSp and Parch are similar features and should be combined and also seem to affect the chance of survival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we established an understanding for the data we can start shaping it into the form we need. For the machine learning algorithms we would want everything to be in numeric values and to be similar in scale. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PassengerId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PassengerId doesn't have influence on the survival chance. In this case we will drop the column from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop the column PassengerID out of the training dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the tickets column (TIPP - look to the command for statistics in the code before)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop the column Ticket out of the training and test dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling the missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cabin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking back at the features, we have 687 missing values in Cabin, 177 values in Age and only 2 in Embarked. <br>\n",
    "The values in Cabin are composed of a letter and a number, with the letter being the deck. Instead of dropping the feature as a whole we will only drop the number and create a new feature called \"deck\", resulting from the letter of the Cabin. The deck letter will be converted into numeric and missing values are 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display the first 8 rows of the feature cabin\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We want to delete the Cabin nomenclature and replace it with a deck array representing the letter of the cabin\n",
    "\n",
    "#import re\n",
    "#deck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\n",
    "#data = [train_df, test_df]\n",
    "\n",
    "#for dataset in data:\n",
    "#    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n",
    "#    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n",
    "#    dataset['Deck'] = dataset['Deck'].map(deck)\n",
    "#    dataset['Deck'] = dataset['Deck'].fillna(0)\n",
    "#    dataset['Deck'] = dataset['Deck'].astype(int)\n",
    "    \n",
    "# Drop the feature cabin from the training and test dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Comparison between Cabin and deck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display the first 8 rows of the new feature Deck\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Age new values will be added derived from the mean age of the standard deviation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fill the missing age data in the training data with a random number between the mean of the age +/- the standard deviation (TIPP - use a for-loop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check if all missing age data are now inserted \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the filled age column from the training dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Embarked feature  has two missing values. These will be filled out with the most common feature. The .describe() function in Python counts all values returns the most top feature. This is only for categorial values. For numeric values the .describe() funcion returns percentiles and mean values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use the .describe() function for the Embarked feature in the training dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Embarked feature the most common one is the port S, with 644 entrires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fill the Embarked feature of the training and test dataset with the most common port 'S'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After filling in all the missing values we can check our dataset with .info(). All features should now contain 891 entires and zero missing values. <br> We can also spot the newly added featues \"not_alone\" and \"Deck\". The imorted image shows the original dataset without the data processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-0e66a4361358>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_df_info.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "train_df.info()\n",
    "print(\"\")\n",
    "img = Image.open('train_df_info.jpg')\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, all features should be numeric values. The next step shows the convertion into numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting \"fare\" from float64 to int 64\n",
    "#data = [train_df, test_df]\n",
    "\n",
    "#for dataset in data:\n",
    "#    dataset['Fare'] = dataset['Fare'].fillna(0)\n",
    "#    dataset['Fare'] = dataset['Fare'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tricky part about the names is that there are many different titles, apart from Mr or Mrs, like \"Lady\", \"Countes\" and so on. These are not high in number but still have to be accounted for. The following titles 'Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer' and 'Dona' will be converted into a single title called 'Rare'. <br>\n",
    "'Mlle' and 'Ms' will turn into 'Miss' and 'Mme' will be 'Mrs'.<br>\n",
    "In the end we are left with five new titles total -> \"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = [train_df, test_df]\n",
    "#titles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
    "\n",
    "#for dataset in data:\n",
    "    # extract titles\n",
    "#    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "    # replace titles with a more common title or as Rare\n",
    "#    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n",
    "#                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "#    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "#    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "#    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
    "    # convert titles into numbers\n",
    "#    dataset['Title'] = dataset['Title'].map(titles)\n",
    "    # filling NaN with 0, to get safe\n",
    "#    dataset['Title'] = dataset['Title'].fillna(0)\n",
    "\n",
    "# Now please drop the name column from the training and test dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again using the .map() function we are defining male as 0 and female as 1. <br>\n",
    "The same is done to convert the ports S, C und Q from the Embarked feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#genders = {\"male\": 0, \"female\": 1}\n",
    "#data = [train_df, test_df]\n",
    "\n",
    "#for dataset in data:\n",
    "#    dataset['Sex'] = dataset['Sex'].map(genders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same with the ports \"S\"= 0, \"C\"= 1, \"Q\"= 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all values have been converted to numeric, we are left with the following train dataset. <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Look, if the training dataset is complete now \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating new categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is very interesting not to have very fine granual data, but split them into even groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cut the Age feature into a group of 8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try this now.\n",
    "\n",
    "#test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now you see how a dataset is  \"renamed\"\n",
    "\n",
    "#Creating Categories\n",
    "#data = [train_df, test_df]\n",
    "#for dataset in data:\n",
    "#    dataset['Age'] = dataset['Age'].astype(int) \n",
    "#    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n",
    "#    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 21), 'Age'] = 1\n",
    "#    dataset.loc[(dataset['Age'] > 21) & (dataset['Age'] <= 24), 'Age'] = 2\n",
    "#    dataset.loc[(dataset['Age'] > 24) & (dataset['Age'] <= 28), 'Age'] = 3\n",
    "#    dataset.loc[(dataset['Age'] > 28) & (dataset['Age'] <= 32), 'Age'] = 4\n",
    "#    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 37), 'Age'] = 5\n",
    "#    dataset.loc[(dataset['Age'] > 37) & (dataset['Age'] <= 45), 'Age'] = 6\n",
    "#    dataset.loc[ dataset['Age'] > 45, 'Age'] = 7\n",
    "\n",
    "# let's see how it's distributed \n",
    "#train_df['Age'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: <br>\n",
    "What feature is left that could also use new categories? <br>\n",
    "\n",
    "Answer\n",
    "\n",
    "??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display the statistical properties of your choosen new category\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the qcut() function, pandas can create ranged groups that have similar amounts of entries.<br>\n",
    "After that we can go through the data and assign the group to each value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do the same as with age for the Fare, first cut the Age feature into a group of 6\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try this now.\n",
    "\n",
    "#groups.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we replace Fare for you also with categories 0 to 5 according to the category Age\n",
    "\n",
    "#data = [train_df, test_df]\n",
    "\n",
    "#for dataset in data:\n",
    "#    dataset.loc[ dataset['Fare'] <= 7, 'Fare'] = 0\n",
    "#    dataset.loc[(dataset['Fare'] > 7) & (dataset['Fare'] <= 8), 'Fare'] = 1\n",
    "#    dataset.loc[(dataset['Fare'] > 8) & (dataset['Fare'] <= 14), 'Fare']   = 2\n",
    "#    dataset.loc[(dataset['Fare'] > 14) & (dataset['Fare'] <= 26), 'Fare']   = 3\n",
    "#    dataset.loc[(dataset['Fare'] > 26) & (dataset['Fare'] <= 52), 'Fare']   = 4\n",
    "#    dataset.loc[ dataset['Fare'] > 52, 'Fare'] = 5\n",
    "#    dataset['Fare'] = dataset['Fare'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "##You see ?\n",
    "\n",
    "#train_df['Fare'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step before we apply the machine learning algorithms, is to create two new features. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = [train_df, test_df]\n",
    "#for dataset in data:\n",
    "#    dataset['Age_Class']= dataset['Age']* dataset['Pclass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for dataset in data:\n",
    "#    dataset['Fare_Per_Person'] = dataset['Fare']/(dataset['relatives']+1)\n",
    "#    dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)# Let's take a last look at the training set, before we start training the models.\n",
    "\n",
    "## Look at the frist 10 rows of your fitted dataset before you start with Machine Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time for Machine Learning - You see, data preparation is quite annoying but essential for Machine Learning. According to the theoretial lecture, we want to predict something (in our case the dataset in the Survive column). Hence, we have to drop the Survive data out of the training dataset and define an \"Answer\" dataset consisting of the Survive dataset placed in the former table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop the Survive data from the trianing dataset und create a new dataset named Y_train with the Survive data\n",
    "\n",
    "\n",
    "##This we have to do, just delete the #\n",
    "#X_test  = test_df.drop(\"PassengerId\", axis=1).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply different Machine Learning algorithms to out preparated data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply the Decision tree algorithm to the training dataset  \n",
    "\n",
    "\n",
    "## We want to evaluate the score of each prediction model, it is also sufficient to round the score to two places\n",
    "#acc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Decent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply the SGD algorithm to the training dataset with a maximum number of iteration = 5 and tol = None\n",
    "\n",
    "\n",
    "## Evaluate the score as for Decision Tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply the Random Forest algorithm to the training dataset with a maximum number of estimators = 100\n",
    "\n",
    "\n",
    "\n",
    "## Evaluate the score as for Random Forest\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply the Logistic Regression algorithm to the training dataset \n",
    "\n",
    "\n",
    "\n",
    "## Evaluate the score as for Logistic Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K nearest neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply the KNN algorithm to the training dataset with 3 neighbors\n",
    "\n",
    "\n",
    "## Evaluate the score as for KNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply the Gaussian Naive Bayes algorithm to the training dataset\n",
    "\n",
    "\n",
    "## Evaluate the score as for Gaussian Naive Bayes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply the Perceptron algorithm to the training dataset with 5 iterations\n",
    "\n",
    "\n",
    "## Evaluate the score as for Perceptron\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply the Linear Support Vector Machine algorithm to the training dataset with 5 iterations\n",
    "\n",
    "\n",
    "## Evaluate the score as for Linear Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In the end of our  first comprehensive Machine Learning example we want the know which model fits best to our problem. \n",
    "## Therefore change, if neccessary, the code according to your given variables for the score\n",
    "\n",
    "#results = pd.DataFrame({\n",
    "#    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n",
    "#              'Random Forest', 'Naive Bayes', 'Perceptron', \n",
    "#              'Stochastic Gradient Decent', \n",
    "#              'Decision Tree'],\n",
    "#    'Score': [acc_linear_svc, acc_knn, acc_log, \n",
    "#              acc_random_forest, acc_gaussian, acc_perceptron, \n",
    "#              acc_sgd, acc_decision_tree]})\n",
    "#result_df = results.sort_values(by='Score', ascending=False)\n",
    "#result_df = result_df.set_index('Score')\n",
    "#result_df.head(9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FHWS vLab Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
